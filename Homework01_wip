---
title: "Ecological Statistics Homework 1"
author: "Kale Hawks"
output:
  html_document: default
  word_document: default
---
# Problem 1: Oak Trees
First I have to load the oak data into my R session.

```{r, fig.width = 6}
url <- "https://pdixon.stat.iastate.edu/stat534/homework/HW%201/bothoak.csv"
rawdata <- read.csv(url, stringsAsFactors = TRUE)
c1 <- rawdata[rawdata$woodland == "dry", "oak"]
c2 <- rawdata[rawdata$woodland == "moist", "oak"]
c0 <- c(c1, c2)
library(emmeans)
library(MASS)
library(mgcv)

```

#### This problem is intended to give you practice working with likelihood and distributions. The data comes from a study of white oaks in two woodlands. One woodland has dry soils; the other has moister soils. Ecology students counted the number of white oak trees in 10m x 10m plots in both types of woodland. There were 22 dry and 18 moist woodland plots. The data are provided in two forms. oakD.csv and oakM.csv have the counts in the dry and moist woodlands, respectively. bothoak.csv has one row for each of the 40 plots. The two variables are the type of woodland (dry, moist) and the number of white oaks. Throughout we will assume that the number of white oak trees in a plot has a Negative Binomial distribution.

### Part A
#### Use the log-likelihood function lnlNB0() defined in the lnlNB.r code on the class website to fit a Negative Binomial distribution to the counts in dry woodland plots. Start the numerical optimization at λ = 20, r = 5, use method=’BFGS’, and turn on trace=T to see the path.

```{r, fig.width = 6}
lnlNB0 <- function(param, y, trace=F) {

  mu <- param[1]
  r <- param[2]
  
  ratio <- r/(r+mu)
  
  lnli <- lgamma(r+y) - lgamma(y+1) - lgamma(r) + r*log(ratio) + y*log(1-ratio)

  
  s = sum(lnli)
  if (trace) {
    print(c(mu, r, s))
    }
  s
}

mu_initial <- 20
r_initial <- 5
starting_parameters <-c(mu_initial, r_initial)

fitc1 <- optim(par = starting_parameters, fn = lnlNB0, y = c1, method = 'BFGS', trace = TRUE)

```

Starting values mu = 20, r = 5
mle mu = 557,544,500,000
mle r = 162,293,400,000
logL = -5,318,621,000,000


#### Does optim() venture “outside” the valid parameter ranges (λ ≥ 0, r > 0) then recover? If so, give the values of μ and r where optim() evaluates lnL but recovers.
Mu and r were always positive, but they were absurdly high. 


#### Does optim() get stuck “outside” the valid parameter ranges?
It just took off at the high end. It was still technically valid, but not plausible.

#### Do you trust the reported mle’s and lnL?
Absolutely not. 

### Part B
#### What would be a more reasonable starting value for λ? (Many answeres are possible, some more reasonable than others). Use optim() with that starting value. If optim() still gives outrageous values, try different choices of r. Report the estimated λ (not log λ) and the NegBin overdispersion parameter.

I tried starting values of mu = 4 and r = 2 , because they seemed closer to what could plausibly be the truth.

This time lambda = 4.587 and overdispersion = -.0008.

Now I'm going to literally try the mean of the data for mu, to see if that works. First I set r to 1, but that returned too many errors, so I chose a higher value of 5. I kept receiving the error: "Error in optim(par = starting_parameters, fn = lnlNB0, y = c1, method = "BFGS",  : 
  non-finite finite-difference value" 

This time mu was dipping into negative values during optimization. But the final value for lambda was 3.769 and the value for overdispersion was -.0005.

Next I tried 
mu_initial <- 50
r_initial <- 60
and made sure to put Hessian = TRUE in my optim() command.
This outputed mu = 2,573,113,107 and r = 2,572,517,637
ln(mu) = 21.668, ln(r) = 21.668

```{r, fig.width = 6}
lnlNB0 <- function(param, y, trace=F) {

  mu <- param[1]
  r <- param[2]
  ratio <- r/(r+mu)
  
  lnli <- lgamma(r+y) - lgamma(y+1) - lgamma(r) + r*log(ratio) + y*log(1-ratio)

  s = sum(lnli)
  if (trace) {
    print(c(mu, r, s))
    }
  s
}

mu_initial <- 50
r_initial <- 60
starting_parameters <-c(mu_initial, r_initial)

fitc1 <- optim(par = starting_parameters, fn = lnlNB0, y = c1, method = 'BFGS', trace = TRUE, hessian = TRUE)
```

Next I tried 

Starting values mu = 7, r = 10

and made sure to put Hessian = TRUE in my optim() command.

This outputed mu = 880.792 and r = 622.148

ln(mu) = 6.78, ln(r) = 6.433

```{r, fig.width = 6}
lnlNB0 <- function(param, y, trace=F) {

  mu <- param[1]
  r <- param[2]
  ratio <- r/(r+mu)
  
  lnli <- lgamma(r+y) - lgamma(y+1) - lgamma(r) + r*log(ratio) + y*log(1-ratio)

  s = sum(lnli)
  if (trace) {
    print(c(mu, r, s))
    }
  s
}

mu_initial <- 7
r_initial <- 10
starting_parameters <-c(mu_initial, r_initial)

fitc1 <- optim(par = starting_parameters, fn = lnlNB0, y = c1, method = 'BFGS', trace = TRUE, hessian = TRUE)
```

Finally, I wondered if I should just flip 20 and 5 (from the first part of the problem), to make it 5 and 20.

Starting values mu = 5, r = 20

mle mu = 15,443,806

mle r = 2,281,301,242

logL = -25,947,053,140

ln(mu) = 16.55
ln(r) = 21.55

```{r, fig.width = 6}
lnlNB0 <- function(param, y, trace=F) {

  mu <- param[1]
  r <- param[2]
  
  ratio <- r/(r+mu)
  
  lnli <- lgamma(r+y) - lgamma(y+1) - lgamma(r) + r*log(ratio) + y*log(1-ratio)

  
  s = sum(lnli)
  if (trace) {
    print(c(mu, r, s))
    }
  s
}

mu_initial <- 5
r_initial <- 20
starting_parameters <-c(mu_initial, r_initial)

fitc1 <- optim(par = starting_parameters, fn = lnlNB0, y = c1, method = 'BFGS', trace = TRUE)

```

### Part C
#### Calculate the standard error of λˆ and the asymptotic 95% confidence interval for the λ. For this confidence interval, use the estimated mean count, λ. and the asymptotic confidence interval equation given in the note summaries.

##### Option 1:
Where I set 50 and 60 for the initial values of mu and r:

se (of mu) = 0.6123334

se (of r) = 1.9409526

Confidence interval for mu:
(2.572575 < mu < 4.972878)

```{r}
fitc1 <- optim(par = starting_parameters, lnlNB0, y=c1, method='BFGS',control=list(fnscale=-1),hessian=T)

vc<- solve(-fitc1$hessian)
cov2cor(vc)
se<-sqrt(diag(vc))

CI <- fitc1$par[1] + c(-1,1)*se[1]*qnorm(0.975)
print(se)
print(CI)

```

Here, where I set 7 and 10 for the initial values of mu and r. 

se (of mu) = 0.6123596

se (of r) = 1.9401993

Confidence interval for mu:
2.572498 < mu < 4.972903

Here, where I set 5 and 20 for the initial values of mu and r. 

se (of mu) = 0.6123693

se (of r) = 1.9389665

Confidence interval for mu:
2.572303 < mu < 4.972747


Overall, the confidence intervals come out almost the same no matter which starting values I began with.

##### Option 2

From when I had too many error messages from using bad starting values:
I'm using the function glm.nb because I couldn't get rid of the error messages using the above method.

```{r, fig.width = 6}
summary(glm.nb(c1 ~ 1))
2*lnlNB0(c(exp(2.7973), 6.11), c1)
```

I used the standard error from this output (1.94) and set it to se. Unfortunately this gives a negative in the lower bound, so it cannot be correct. 

CI: -.0296 < mu < 7.575

```{r, fig.width = 6}
se <- 1.94
fitc1$par[1] + c(-1, 1)*se[1]*qnorm(0.975)

rbind(lower=fitc1$par - se*qnorm(0.975), upper=fitc1$par + se*qnorm(0.975))
```

### Part D
#### Find the mle’s when the parameters are log λ and r. Is the log likelihood for this fit the same as when you use λ as the parameter?

##### Original method:
Starting values mu = 5, r = 20

mle mu = 15,443,806

mle r 2,281,301,242

logL = -25,947,053,140

ln(mu) = 16.55
ln(r) = 21.55

##### New method: 
Starting values: 5, 20
mle mu = 5.944
mle r = 1.3196
logL = -51.314

This new method is much better, because I feel I can actually interpret these values. 

```{r, fig.width = 6}
lnlNB <- function(param, y, trace=F) {

  mu <- exp(param[1])
  r <- exp(param[2])
  
  lnli = dnbinom(y, size=r, mu=mu, log=T)

  s = sum(lnli)
  if (trace) {
    print(c(mu, r, s))
    }
  s
}

fit_newC1 <- optim(log(c(5, 20)), lnlNB, y = c2, method='BFGS',
  control=list(fnscale=-1), trace = TRUE, hessian = TRUE)
```

### Part E
#### Estimate the log scale mean number of oaks in a dry woodland plot and the standard error of the log-scale estimate. Use the log mean and the asymptotic confidence interval formula to compute a 95% confidence interval for the log mean. Backtransform the log scale confidence interval to provide a 95% confidence interval for λ.

My estimate for mu is 5.94.
se = 0.2266325
CI: 3.812 < mu < 9.269

```{r, fig.width = 6}
lnlNB <- function(param, y, trace=F) {

  mu <- exp(param[1])
  r <- exp(param[2])
  
  lnli = dnbinom(y, size=r, mu=mu, log=T)

  s = sum(lnli)
  if (trace) {
    print(c(mu, r, s))
    }
  s
}

fit_newC1 <- optim(log(c(5, 20)), lnlNB, y = c2, method='BFGS',
  control=list(fnscale=-1), trace = FALSE, hessian = TRUE)

vc<- solve(-fit_newC1$hessian)
cov2cor(vc)
se<-sqrt(diag(vc))

fit_newC1$par[1] + c(-1,1)*se[1]*qnorm(0.975)

transformed <- exp(fit_newC1$par)
print(transformed)

CI <- fit_newC1$par[1] + c(-1,1)*se[1]*qnorm(0.975)

exp(CI)
```

### Part F
#### Use a likelihood ratio test to test the null hypothesis of no difference in mean counts between the dry and the moist woodland. Continue to assume that the counts are approximately Negative Binomial and assume that the two woodlands have the same overdispersion parameter. Report the test statistic (twice the change in log likelihood) and the p-value.

c1 : Dry woodlands: 22 observations, mean = 3.773
c2 : Moist woodlands: 18 observations, mean = 5.9444
c0 : Combined: 40 observations, mean = 4.75

I created a confidence interval for the combined data set, with both the moist and the dry woodlands (n = 40), using the log-scale method from part E to visually check whether both means (for dry and for moist) fell within the combined CI. This yielded:

mu = 4.75

se = 0.1419

CI: 3.597 < mu < 6.273.

```{r, fig.width = 6}
y <- c0

lnlNB <- function(param, y, trace=F) {

  mu <- exp(param[1])
  r <- exp(param[2])
  
  lnli = dnbinom(y, size=r, mu=mu, log=T)

  s = sum(lnli)
  if (trace) {
    print(c(mu, r, s))
    }
  s
}

fitc0 <- optim(log(c(5, 20)), lnlNB, y = c0, method='BFGS',
  control=list(fnscale=-1), trace = TRUE, hessian = TRUE)

vc<- solve(-fitc0$hessian)
cov2cor(vc)
se<-sqrt(diag(vc))

fitc0$par[1] + c(-1,1)*se[1]*qnorm(0.975)

transformed <- exp(fitc0$par)
print(transformed)
print(se)

CI <- fitc0$par[1] + c(-1,1)*se[1]*qnorm(0.975)

transformed_CI <- exp(CI)

print(CI)
print(transformed_CI)
```

Attempt at doing the hypothesis test to analyze change in oak abundance between the dry and moist woodlands:

I wanted to do a t-test first, because I'm familiar with it and understand how to interpret it: 

t = -1.6541, df = 25.926, p-value = 0.1102

```{r, fig.width = 6}

url <- "https://pdixon.stat.iastate.edu/stat534/homework/HW%201/bothoak.csv"
rawdata <- read.csv(url, stringsAsFactors = TRUE)
t_test_result <- t.test(oak ~ woodland, data = rawdata)
print(t_test_result)
```


Next, the loglikelihood difference. I set up the null hypothesis (that the means are the same for both groups, the whole data set) and the alternative hypothesis (that the means for dry and moist are different). 

null lnL = -128.239

alt lnL = -123.3563

test statistic = 9.765353

p = 0.001778314

With this p value I reject the null hypothesis and conclude that the two types of woodlands have different means of oak trees. Notice that this is a stronger result than my simple t test above. 

```{r, fig.width = 6}
l0 <- mean(c0)
l1 <- mean(c1)
l2 <- mean(c2)

null <- sum(-l0 + c0*log(l0) - lfactorial(c0))

alt <- sum(-l1 + c1*log(l1) - lfactorial(c1)) + sum(-l2 + c2*log(l2) - lfactorial(c2))

test_statistic <- -2 * (null - alt)
p_value <- 1 - pchisq(test_statistic, df = 1)

print(null)
print(alt)
print(test_statistic)
print(p_value)

```
